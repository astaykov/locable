services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama

    # GPU support (works on Windows + Linux)
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    volumes:
      - ollama_models:/root/.ollama

    ports:
      - "11434:11434"

    restart: unless-stopped

    entrypoint:
      - sh
      - -c
      - |
        echo "Starting Ollama with GPU support..."
        ollama serve &
        sleep 2
        echo "Pulling model qwen..."
        ollama pull qwen2.5-coder:14b-instruct
        wait

  locable:
    image: ghcr.io/pstaykov/locable:latest
    container_name: locable

    environment:
      OLLAMA_HOST: "http://ollama:11434"

    ports:
      - "8000:8000"

    depends_on:
      - ollama

volumes:
  ollama_models:
